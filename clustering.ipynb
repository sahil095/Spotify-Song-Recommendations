{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# K-Means Clustering Analysis\n",
        "\n",
        "This notebook performs clustering analysis on Spotify songs dataset using K-Means clustering algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# IMPORTS AND SETUP\n",
        "# ============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import silhouette_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for better visualizations\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "DATA LOADING\n",
            "================================================================================\n",
            "\n",
            "Dataset shape: (85000, 32)\n",
            "Columns: ['track_id', 'track_name', 'artist_name', 'album_name', 'release_date', 'genre', 'duration_ms', 'popularity', 'danceability', 'energy', 'key', 'loudness', 'mode', 'instrumentalness', 'tempo', 'stream_count', 'country', 'explicit', 'label', 'release_year', 'release_month', 'release_quarter', 'release_week', 'release_day_of_week', 'duration_min', 'popularity_bin', 'stream_category', 'key_label', 'energy_danceability', 'popularity_energy', 'instrumentalness_energy', 'mode_label']\n",
            "\n",
            "First few rows:\n",
            "           track_id              track_name    artist_name         album_name  \\\n",
            "0  TRK-BEBD53DA84E1         Agent every (0)    Noah Rhodes  Beautiful instead   \n",
            "1  TRK-6A32496762D7           Night respond  Jennifer Cole              Table   \n",
            "2  TRK-47AA7523463E  Future choice whatever  Brandon Davis      Page southern   \n",
            "3  TRK-25ADA22E3B06     Bad fall pick those    Corey Jones             Spring   \n",
            "4  TRK-9245F2AD996A                 Husband      Mark Diaz        Great prove   \n",
            "\n",
            "  release_date  genre  duration_ms  popularity  danceability  energy  ...  \\\n",
            "0   2016-04-01    Pop       234194          55          0.15    0.74  ...   \n",
            "1   2022-04-15  Metal       375706          45          0.44    0.46  ...   \n",
            "2   2016-02-23   Rock       289191          55          0.62    0.80  ...   \n",
            "3   2015-10-12    Pop       209484          51          0.78    0.98  ...   \n",
            "4   2022-07-08  Indie       127435          39          0.74    0.18  ...   \n",
            "\n",
            "   release_week  release_day_of_week  duration_min  popularity_bin  \\\n",
            "0            13               Friday      3.903233          medium   \n",
            "1            15               Friday      6.261767          medium   \n",
            "2             8              Tuesday      4.819850          medium   \n",
            "3            42               Monday      3.491400          medium   \n",
            "4            27               Friday      2.123917          medium   \n",
            "\n",
            "   stream_category  key_label energy_danceability  popularity_energy  \\\n",
            "0       low_stream          A              0.1110             0.4070   \n",
            "1       low_stream          C              0.2024             0.2070   \n",
            "2       low_stream         G#              0.4960             0.4400   \n",
            "3       low_stream         C#              0.7644             0.4998   \n",
            "4       low_stream         A#              0.1332             0.0702   \n",
            "\n",
            "  instrumentalness_energy  mode_label  \n",
            "0                 0.32264       Minor  \n",
            "1                 0.10258       Minor  \n",
            "2                 0.46720       Major  \n",
            "3                 0.67032       Major  \n",
            "4                 0.05472       Minor  \n",
            "\n",
            "[5 rows x 32 columns]\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# STEP 1: LOAD DATA\n",
        "# ============================================================================\n",
        "\n",
        "df_clean_path = r\"D:\\UNH Materials\\Projects\\Spotify Song Recommendations\\data\\df_clean.csv\"\n",
        "df_clean = pd.read_csv(df_clean_path)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"DATA LOADING\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nDataset shape: {df_clean.shape}\")\n",
        "print(f\"Columns: {df_clean.columns.tolist()}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "print(df_clean.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "FEATURE SELECTION\n",
            "================================================================================\n",
            "\n",
            "Selected Numerical Features (10):\n",
            "  - danceability\n",
            "  - energy\n",
            "  - key\n",
            "  - loudness\n",
            "  - mode\n",
            "  - instrumentalness\n",
            "  - tempo\n",
            "  - duration_min\n",
            "  - popularity\n",
            "  - explicit\n",
            "\n",
            "Selected Categorical Features (2):\n",
            "  - country\n",
            "  - label\n",
            "\n",
            "Feature matrix shape: (85000, 12)\n",
            "Missing values: 0\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# STEP 2: SELECT OPTIMAL FEATURES FOR CLUSTERING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"FEATURE SELECTION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Select optimal features for clustering\n",
        "# Audio features that capture musical characteristics\n",
        "numerical_features = [\n",
        "    'danceability',      # How suitable a track is for dancing\n",
        "    'energy',            # Perceptual measure of intensity and power\n",
        "    'key',               # Key the track is in\n",
        "    'loudness',          # Overall loudness in decibels\n",
        "    'mode',              # Major (1) or minor (0)\n",
        "    'instrumentalness', # Predicts whether a track contains vocals\n",
        "    'tempo',             # Overall estimated tempo in BPM\n",
        "    'duration_min',      # Duration in minutes\n",
        "    'popularity',       # Popularity score (0-100)\n",
        "    'explicit'          # Whether the track contains explicit content\n",
        "]\n",
        "\n",
        "# Categorical features that might help distinguish clusters\n",
        "categorical_features = [\n",
        "    'country',           # Country of origin\n",
        "    'label'              # Record label\n",
        "]\n",
        "\n",
        "# Check which features exist in the dataset\n",
        "available_numerical = [f for f in numerical_features if f in df_clean.columns]\n",
        "available_categorical = [f for f in categorical_features if f in df_clean.columns]\n",
        "\n",
        "print(f\"\\nSelected Numerical Features ({len(available_numerical)}):\")\n",
        "for feat in available_numerical:\n",
        "    print(f\"  - {feat}\")\n",
        "\n",
        "print(f\"\\nSelected Categorical Features ({len(available_categorical)}):\")\n",
        "for feat in available_categorical:\n",
        "    print(f\"  - {feat}\")\n",
        "\n",
        "# Create feature matrix\n",
        "X = df_clean[available_numerical + available_categorical].copy()\n",
        "\n",
        "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
        "print(f\"Missing values: {X.isnull().sum().sum()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "ENCODING CATEGORICAL FEATURES\n",
            "================================================================================\n",
            "\n",
            "Numerical features shape: (85000, 10)\n",
            "Categorical features shape: (85000, 2)\n",
            "\n",
            "Applying One-Hot Encoding to 2 categorical features...\n",
            "  Original categorical columns: 2\n",
            "  Encoded categorical columns: 18\n",
            "  'country': 10 unique values\n",
            "  'label': 8 unique values\n",
            "\n",
            "Final feature matrix shape after encoding: (85000, 28)\n",
            "Total features: 28\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# STEP 3: ENCODE CATEGORICAL FEATURES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"ENCODING CATEGORICAL FEATURES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Separate numerical and categorical columns\n",
        "X_numerical = X[available_numerical].copy()\n",
        "X_categorical = X[available_categorical].copy()\n",
        "\n",
        "print(f\"\\nNumerical features shape: {X_numerical.shape}\")\n",
        "print(f\"Categorical features shape: {X_categorical.shape}\")\n",
        "\n",
        "# Apply One-Hot Encoding to categorical features\n",
        "if len(available_categorical) > 0:\n",
        "    print(f\"\\nApplying One-Hot Encoding to {len(available_categorical)} categorical features...\")\n",
        "    \n",
        "    ohe = OneHotEncoder(sparse_output=False, drop=None, handle_unknown='ignore')\n",
        "    X_categorical_encoded = ohe.fit_transform(X_categorical)\n",
        "    categorical_feature_names = ohe.get_feature_names_out(available_categorical)\n",
        "    \n",
        "    print(f\"  Original categorical columns: {len(available_categorical)}\")\n",
        "    print(f\"  Encoded categorical columns: {len(categorical_feature_names)}\")\n",
        "    \n",
        "    # Combine numerical and encoded categorical features\n",
        "    X_encoded = pd.DataFrame(\n",
        "        data=np.hstack([X_numerical.values, X_categorical_encoded]),\n",
        "        columns=list(available_numerical) + list(categorical_feature_names),\n",
        "        index=X.index\n",
        "    )\n",
        "    \n",
        "    # Print encoding details\n",
        "    for col in available_categorical:\n",
        "        print(f\"  '{col}': {X[col].nunique()} unique values\")\n",
        "else:\n",
        "    X_encoded = X_numerical.copy()\n",
        "\n",
        "print(f\"\\nFinal feature matrix shape after encoding: {X_encoded.shape}\")\n",
        "print(f\"Total features: {X_encoded.shape[1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "FEATURE SCALING\n",
            "================================================================================\n",
            "\n",
            "Scaling completed!\n",
            "Scaled feature matrix shape: (85000, 28)\n",
            "\n",
            "Scaled feature statistics:\n",
            "       danceability     energy        key   loudness     mode  \\\n",
            "count     85000.000  85000.000  85000.000  85000.000  85000.0   \n",
            "mean         -0.000     -0.000     -0.000      0.000      0.0   \n",
            "std           1.000      1.000      1.000      1.000      1.0   \n",
            "min          -1.738     -1.735     -1.597     -1.731     -1.0   \n",
            "25%          -0.852     -0.877     -0.801     -0.864     -1.0   \n",
            "50%          -0.003     -0.019      0.140      0.002      1.0   \n",
            "75%           0.884      0.874      1.009      0.865      1.0   \n",
            "max           1.733      1.732      1.588      1.731      1.0   \n",
            "\n",
            "       instrumentalness      tempo  duration_min  popularity   explicit  ...  \\\n",
            "count         85000.000  85000.000     85000.000   85000.000  85000.000  ...   \n",
            "mean             -0.000      0.000         0.000       0.000      0.000  ...   \n",
            "std               1.000      1.000         1.000       1.000      1.000  ...   \n",
            "min              -1.727     -1.730        -1.730      -3.248     -0.502  ...   \n",
            "25%              -0.872     -0.868        -0.871      -0.685     -0.502  ...   \n",
            "50%              -0.003      0.001         0.000      -0.078     -0.502  ...   \n",
            "75%               0.865      0.867         0.863       0.596     -0.502  ...   \n",
            "max               1.729      1.732         1.732       3.496      1.992  ...   \n",
            "\n",
            "       country_United Kingdom  country_United States  label_Columbia  \\\n",
            "count               85000.000              85000.000       85000.000   \n",
            "mean                   -0.000                  0.000          -0.000   \n",
            "std                     1.000                  1.000           1.000   \n",
            "min                    -0.332                 -0.330          -0.379   \n",
            "25%                    -0.332                 -0.330          -0.379   \n",
            "50%                    -0.332                 -0.330          -0.379   \n",
            "75%                    -0.332                 -0.330          -0.379   \n",
            "max                     3.008                  3.028           2.637   \n",
            "\n",
            "       label_EMI  label_Independent  label_Island Records  label_Sony Music  \\\n",
            "count  85000.000          85000.000             85000.000         85000.000   \n",
            "mean       0.000              0.000                 0.000            -0.000   \n",
            "std        1.000              1.000                 1.000             1.000   \n",
            "min       -0.377             -0.381                -0.376            -0.378   \n",
            "25%       -0.377             -0.381                -0.376            -0.378   \n",
            "50%       -0.377             -0.381                -0.376            -0.378   \n",
            "75%       -0.377             -0.381                -0.376            -0.378   \n",
            "max        2.653              2.626                 2.660             2.645   \n",
            "\n",
            "       label_Universal Music  label_Warner Music  label_XL Recordings  \n",
            "count              85000.000           85000.000            85000.000  \n",
            "mean                  -0.000               0.000               -0.000  \n",
            "std                    1.000               1.000                1.000  \n",
            "min                   -0.376              -0.378               -0.379  \n",
            "25%                   -0.376              -0.378               -0.379  \n",
            "50%                   -0.376              -0.378               -0.379  \n",
            "75%                   -0.376              -0.378               -0.379  \n",
            "max                    2.663               2.647                2.637  \n",
            "\n",
            "[8 rows x 28 columns]\n",
            "\n",
            "================================================================================\n",
            "DATA PREPARATION COMPLETE\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# STEP 4: SCALE FEATURES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"FEATURE SCALING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# StandardScaler normalizes features to have mean=0 and std=1\n",
        "# This is crucial for K-means as it uses Euclidean distance\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_encoded)\n",
        "\n",
        "# Convert back to DataFrame for easier handling\n",
        "X_scaled_df = pd.DataFrame(\n",
        "    data=X_scaled,\n",
        "    columns=X_encoded.columns,\n",
        "    index=X_encoded.index\n",
        ")\n",
        "\n",
        "print(f\"\\nScaling completed!\")\n",
        "print(f\"Scaled feature matrix shape: {X_scaled_df.shape}\")\n",
        "print(f\"\\nScaled feature statistics:\")\n",
        "print(X_scaled_df.describe().round(3))\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DATA PREPARATION COMPLETE\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "ELBOW METHOD: FINDING OPTIMAL NUMBER OF CLUSTERS\n",
            "================================================================================\n",
            "\n",
            "Testing k values from 2 to 10...\n",
            "This may take a few minutes...\n",
            "  Testing k=2... ✓ (Inertia: 2282825.26, Silhouette: 0.044)\n",
            "  Testing k=3... ✓ (Inertia: 2200400.66, Silhouette: 0.060)\n",
            "  Testing k=4... ✓ (Inertia: 2118337.06, Silhouette: 0.074)\n",
            "  Testing k=5... ✓ (Inertia: 2045044.33, Silhouette: 0.085)\n",
            "  Testing k=6... ✓ (Inertia: 1950285.51, Silhouette: 0.109)\n",
            "  Testing k=7... ✓ (Inertia: 1813089.90, Silhouette: 0.153)\n",
            "  Testing k=8... ✓ (Inertia: 1718680.34, Silhouette: 0.174)\n",
            "  Testing k=9... ✓ (Inertia: 1624155.10, Silhouette: 0.196)\n",
            "  Testing k=10... ✓ (Inertia: 1529846.84, Silhouette: 0.218)\n",
            "\n",
            "Elbow method computation completed!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# STEP 5: ELBOW METHOD TO FIND OPTIMAL NUMBER OF CLUSTERS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"ELBOW METHOD: FINDING OPTIMAL NUMBER OF CLUSTERS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Range of k values to test\n",
        "k_range = range(2, 11)  # Testing k from 2 to 10\n",
        "inertias = []\n",
        "silhouette_scores = []\n",
        "\n",
        "print(f\"\\nTesting k values from {min(k_range)} to {max(k_range)}...\")\n",
        "print(\"This may take a few minutes...\")\n",
        "\n",
        "for k in k_range:\n",
        "    print(f\"  Testing k={k}...\", end=\" \")\n",
        "    \n",
        "    # Initialize and fit K-means\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10, max_iter=300)\n",
        "    kmeans.fit(X_scaled)\n",
        "    \n",
        "    # Calculate inertia (within-cluster sum of squares)\n",
        "    inertias.append(kmeans.inertia_)\n",
        "    \n",
        "    # Calculate silhouette score\n",
        "    labels = kmeans.labels_\n",
        "    silhouette_avg = silhouette_score(X_scaled, labels)\n",
        "    silhouette_scores.append(silhouette_avg)\n",
        "    \n",
        "    print(f\"✓ (Inertia: {kmeans.inertia_:.2f}, Silhouette: {silhouette_avg:.3f})\")\n",
        "\n",
        "print(\"\\nElbow method computation completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# STEP 6: VISUALIZE ELBOW METHOD RESULTS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"ELBOW METHOD VISUALIZATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create figure with two subplots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Plot 1: Elbow Curve (Inertia)\n",
        "ax1 = axes[0]\n",
        "ax1.plot(k_range, inertias, 'bo-', linewidth=2, markersize=8, label='Inertia')\n",
        "ax1.set_xlabel('Number of Clusters (k)', fontsize=12, fontweight='bold')\n",
        "ax1.set_ylabel('Inertia (Within-Cluster Sum of Squares)', fontsize=12, fontweight='bold')\n",
        "ax1.set_title('Elbow Method: Finding Optimal k', fontsize=14, fontweight='bold', pad=15)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.legend(fontsize=11)\n",
        "\n",
        "# Add value labels on points\n",
        "for k, inertia in zip(k_range, inertias):\n",
        "    ax1.text(k, inertia, f'{inertia:.0f}', ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "# Plot 2: Silhouette Score\n",
        "ax2 = axes[1]\n",
        "ax2.plot(k_range, silhouette_scores, 'ro-', linewidth=2, markersize=8, label='Silhouette Score')\n",
        "ax2.set_xlabel('Number of Clusters (k)', fontsize=12, fontweight='bold')\n",
        "ax2.set_ylabel('Silhouette Score', fontsize=12, fontweight='bold')\n",
        "ax2.set_title('Silhouette Score: Quality of Clustering', fontsize=14, fontweight='bold', pad=15)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.legend(fontsize=11)\n",
        "ax2.set_ylim([0, max(silhouette_scores) * 1.1])\n",
        "\n",
        "# Add value labels on points\n",
        "for k, score in zip(k_range, silhouette_scores):\n",
        "    ax2.text(k, score, f'{score:.3f}', ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "plt.suptitle('Determining Optimal Number of Clusters', fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find optimal k based on elbow method and silhouette score\n",
        "# Elbow: Look for the \"elbow\" where the rate of decrease sharply changes\n",
        "# Silhouette: Higher is better (closer to 1 is best)\n",
        "\n",
        "optimal_k_silhouette = k_range[np.argmax(silhouette_scores)]\n",
        "max_silhouette = max(silhouette_scores)\n",
        "\n",
        "print(f\"\\nOptimal k based on Silhouette Score: {optimal_k_silhouette} (Score: {max_silhouette:.3f})\")\n",
        "print(f\"\\nInertia values:\")\n",
        "for k, inertia in zip(k_range, inertias):\n",
        "    print(f\"  k={k:2d}: {inertia:.2f}\")\n",
        "\n",
        "print(f\"\\nSilhouette scores:\")\n",
        "for k, score in zip(k_range, silhouette_scores):\n",
        "    print(f\"  k={k:2d}: {score:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# STEP 7: DETERMINE OPTIMAL K\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"DETERMINING OPTIMAL NUMBER OF CLUSTERS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Method 1: Elbow method - calculate rate of change in inertia\n",
        "inertia_changes = np.diff(inertias)\n",
        "inertia_change_rates = np.diff(inertia_changes)\n",
        "\n",
        "# Find the elbow (where the rate of change is maximum)\n",
        "# We look for the point where the decrease in inertia slows down significantly\n",
        "elbow_idx = np.argmax(np.abs(inertia_change_rates)) + 2  # +2 because of double diff\n",
        "optimal_k_elbow = k_range[elbow_idx] if elbow_idx < len(k_range) else optimal_k_silhouette\n",
        "\n",
        "# Method 2: Silhouette score (already calculated)\n",
        "optimal_k_silhouette = k_range[np.argmax(silhouette_scores)]\n",
        "\n",
        "# Use silhouette-based optimal k (generally more reliable)\n",
        "optimal_k = optimal_k_silhouette\n",
        "\n",
        "print(f\"\\nAnalysis Results:\")\n",
        "print(f\"  Elbow method suggests: k = {optimal_k_elbow}\")\n",
        "print(f\"  Silhouette method suggests: k = {optimal_k_silhouette}\")\n",
        "print(f\"\\n  Selected optimal k: {optimal_k} (based on Silhouette Score)\")\n",
        "print(f\"  Silhouette Score at k={optimal_k}: {silhouette_scores[optimal_k - min(k_range)]:.3f}\")\n",
        "print(f\"  Inertia at k={optimal_k}: {inertias[optimal_k - min(k_range)]:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# STEP 8: PERFORM K-MEANS CLUSTERING WITH OPTIMAL K\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(f\"PERFORMING K-MEANS CLUSTERING WITH k={optimal_k}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Initialize and fit K-means with optimal k\n",
        "kmeans_final = KMeans(\n",
        "    n_clusters=optimal_k,\n",
        "    random_state=42,\n",
        "    n_init=10,  # Number of times to run with different centroid seeds\n",
        "    max_iter=300  # Maximum number of iterations\n",
        ")\n",
        "\n",
        "print(f\"\\nFitting K-means model with k={optimal_k}...\")\n",
        "kmeans_final.fit(X_scaled)\n",
        "\n",
        "# Get cluster labels\n",
        "cluster_labels = kmeans_final.labels_\n",
        "\n",
        "# Add cluster labels to original dataframe\n",
        "df_clean['cluster'] = cluster_labels\n",
        "\n",
        "print(f\"\\nClustering completed!\")\n",
        "print(f\"\\nCluster distribution:\")\n",
        "cluster_counts = pd.Series(cluster_labels).value_counts().sort_index()\n",
        "for cluster_id, count in cluster_counts.items():\n",
        "    percentage = (count / len(cluster_labels)) * 100\n",
        "    print(f\"  Cluster {cluster_id}: {count:,} songs ({percentage:.2f}%)\")\n",
        "\n",
        "# Calculate final metrics\n",
        "final_inertia = kmeans_final.inertia_\n",
        "final_silhouette = silhouette_score(X_scaled, cluster_labels)\n",
        "\n",
        "print(f\"\\nFinal Clustering Metrics:\")\n",
        "print(f\"  Inertia: {final_inertia:.2f}\")\n",
        "print(f\"  Silhouette Score: {final_silhouette:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# STEP 9: CLUSTER ANALYSIS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"CLUSTER CHARACTERISTICS ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Analyze cluster characteristics using original numerical features\n",
        "cluster_analysis = df_clean.groupby('cluster')[available_numerical].mean()\n",
        "\n",
        "print(f\"\\nAverage feature values by cluster:\")\n",
        "print(\"-\"*80)\n",
        "print(cluster_analysis.round(3).to_string())\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Analyze genre distribution in each cluster\n",
        "if 'genre' in df_clean.columns:\n",
        "    print(f\"\\nGenre distribution by cluster:\")\n",
        "    genre_cluster = pd.crosstab(df_clean['cluster'], df_clean['genre'], normalize='index') * 100\n",
        "    print(genre_cluster.round(2).to_string())\n",
        "    print(\"-\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# STEP 10: VISUALIZE CLUSTERS USING PCA\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"PCA VISUALIZATION OF CLUSTERS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Apply PCA to reduce dimensions to 2D for visualization\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Calculate explained variance\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "total_variance = explained_variance.sum()\n",
        "\n",
        "print(f\"\\nPCA Results:\")\n",
        "print(f\"  PC1 explains {explained_variance[0]*100:.2f}% of variance\")\n",
        "print(f\"  PC2 explains {explained_variance[1]*100:.2f}% of variance\")\n",
        "print(f\"  Total variance explained: {total_variance*100:.2f}%\")\n",
        "\n",
        "# Create visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
        "\n",
        "# Plot 1: Clusters colored by cluster ID\n",
        "ax1 = axes[0]\n",
        "scatter = ax1.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, \n",
        "                     cmap='tab20', alpha=0.6, s=20, edgecolors='black', linewidth=0.3)\n",
        "ax1.set_xlabel(f'First Principal Component (PC1) - {explained_variance[0]*100:.1f}% variance', \n",
        "               fontsize=12, fontweight='bold')\n",
        "ax1.set_ylabel(f'Second Principal Component (PC2) - {explained_variance[1]*100:.1f}% variance', \n",
        "               fontsize=12, fontweight='bold')\n",
        "ax1.set_title(f'K-Means Clustering Visualization (k={optimal_k})\\nPCA Projection', \n",
        "              fontsize=14, fontweight='bold', pad=15)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "plt.colorbar(scatter, ax=ax1, label='Cluster ID')\n",
        "\n",
        "# Plot cluster centers in PCA space\n",
        "cluster_centers_pca = pca.transform(kmeans_final.cluster_centers_)\n",
        "ax1.scatter(cluster_centers_pca[:, 0], cluster_centers_pca[:, 1], \n",
        "           c='red', marker='X', s=300, edgecolors='black', linewidth=2, \n",
        "           label='Cluster Centers', zorder=5)\n",
        "ax1.legend()\n",
        "\n",
        "# Plot 2: Clusters colored by genre (if available)\n",
        "ax2 = axes[1]\n",
        "if 'genre' in df_clean.columns:\n",
        "    # Map genres to colors\n",
        "    unique_genres = sorted(df_clean['genre'].unique())\n",
        "    genre_colors = plt.cm.Set3(np.linspace(0, 1, len(unique_genres)))\n",
        "    genre_color_map = dict(zip(unique_genres, genre_colors))\n",
        "    \n",
        "    colors_by_genre = [genre_color_map[g] for g in df_clean['genre']]\n",
        "    \n",
        "    scatter2 = ax2.scatter(X_pca[:, 0], X_pca[:, 1], c=colors_by_genre, \n",
        "                         alpha=0.6, s=20, edgecolors='black', linewidth=0.3)\n",
        "    ax2.set_xlabel(f'First Principal Component (PC1) - {explained_variance[0]*100:.1f}% variance', \n",
        "                  fontsize=12, fontweight='bold')\n",
        "    ax2.set_ylabel(f'Second Principal Component (PC2) - {explained_variance[1]*100:.1f}% variance', \n",
        "                  fontsize=12, fontweight='bold')\n",
        "    ax2.set_title('PCA Projection Colored by Genre', \n",
        "                 fontsize=14, fontweight='bold', pad=15)\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Add legend for genres (sample)\n",
        "    from matplotlib.patches import Patch\n",
        "    legend_elements = [Patch(facecolor=genre_color_map[g], label=g) for g in unique_genres[:12]]\n",
        "    ax2.legend(handles=legend_elements, bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
        "else:\n",
        "    ax2.text(0.5, 0.5, 'Genre information not available', \n",
        "            ha='center', va='center', transform=ax2.transAxes, fontsize=12)\n",
        "    ax2.set_title('PCA Projection', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.suptitle('K-Means Clustering: 2D PCA Visualization', \n",
        "             fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nVisualization completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# STEP 11: ADDITIONAL CLUSTER VISUALIZATIONS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"ADDITIONAL CLUSTER ANALYSIS VISUALIZATIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create a comprehensive visualization\n",
        "fig = plt.figure(figsize=(20, 12))\n",
        "\n",
        "# 1. Cluster size distribution\n",
        "ax1 = plt.subplot(2, 3, 1)\n",
        "cluster_counts = pd.Series(cluster_labels).value_counts().sort_index()\n",
        "bars = ax1.bar(cluster_counts.index, cluster_counts.values, \n",
        "               color=plt.cm.tab20(np.linspace(0, 1, len(cluster_counts))), \n",
        "               edgecolor='black', alpha=0.8)\n",
        "ax1.set_xlabel('Cluster ID', fontsize=11, fontweight='bold')\n",
        "ax1.set_ylabel('Number of Songs', fontsize=11, fontweight='bold')\n",
        "ax1.set_title('Cluster Size Distribution', fontsize=12, fontweight='bold')\n",
        "ax1.grid(axis='y', alpha=0.3)\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
        "             f'{int(height)}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# 2. Genre distribution in clusters (heatmap)\n",
        "ax2 = plt.subplot(2, 3, 2)\n",
        "if 'genre' in df_clean.columns:\n",
        "    genre_cluster_crosstab = pd.crosstab(df_clean['cluster'], df_clean['genre'])\n",
        "    sns.heatmap(genre_cluster_crosstab, annot=True, fmt='d', cmap='YlOrRd', \n",
        "                ax=ax2, cbar_kws={'label': 'Count'})\n",
        "    ax2.set_xlabel('Genre', fontsize=11, fontweight='bold')\n",
        "    ax2.set_ylabel('Cluster', fontsize=11, fontweight='bold')\n",
        "    ax2.set_title('Genre Distribution Across Clusters', fontsize=12, fontweight='bold')\n",
        "    plt.setp(ax2.get_xticklabels(), rotation=45, ha='right')\n",
        "\n",
        "# 3. Average audio features by cluster (heatmap)\n",
        "ax3 = plt.subplot(2, 3, 3)\n",
        "cluster_features = df_clean.groupby('cluster')[available_numerical[:6]].mean()\n",
        "sns.heatmap(cluster_features.T, annot=True, fmt='.2f', cmap='RdYlBu_r', \n",
        "            ax=ax3, cbar_kws={'label': 'Average Value'}, center=0)\n",
        "ax3.set_xlabel('Cluster', fontsize=11, fontweight='bold')\n",
        "ax3.set_ylabel('Audio Feature', fontsize=11, fontweight='bold')\n",
        "ax3.set_title('Average Audio Features by Cluster', fontsize=12, fontweight='bold')\n",
        "\n",
        "# 4. PCA with cluster centers\n",
        "ax4 = plt.subplot(2, 3, 4)\n",
        "for cluster_id in range(optimal_k):\n",
        "    mask = cluster_labels == cluster_id\n",
        "    ax4.scatter(X_pca[mask, 0], X_pca[mask, 1], \n",
        "               label=f'Cluster {cluster_id}', alpha=0.6, s=30, edgecolors='black', linewidth=0.3)\n",
        "ax4.scatter(cluster_centers_pca[:, 0], cluster_centers_pca[:, 1], \n",
        "           c='red', marker='X', s=400, edgecolors='black', linewidth=2, \n",
        "           label='Centroids', zorder=5)\n",
        "ax4.set_xlabel(f'PC1 ({explained_variance[0]*100:.1f}%)', fontsize=11, fontweight='bold')\n",
        "ax4.set_ylabel(f'PC2 ({explained_variance[1]*100:.1f}%)', fontsize=11, fontweight='bold')\n",
        "ax4.set_title('Clusters in PCA Space', fontsize=12, fontweight='bold')\n",
        "ax4.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "# 5. Popularity distribution by cluster\n",
        "ax5 = plt.subplot(2, 3, 5)\n",
        "if 'popularity' in df_clean.columns:\n",
        "    cluster_popularity = df_clean.groupby('cluster')['popularity'].mean()\n",
        "    bars = ax5.bar(cluster_popularity.index, cluster_popularity.values,\n",
        "                   color=plt.cm.viridis(np.linspace(0, 1, len(cluster_popularity))),\n",
        "                   edgecolor='black', alpha=0.8)\n",
        "    ax5.set_xlabel('Cluster ID', fontsize=11, fontweight='bold')\n",
        "    ax5.set_ylabel('Average Popularity', fontsize=11, fontweight='bold')\n",
        "    ax5.set_title('Average Popularity by Cluster', fontsize=12, fontweight='bold')\n",
        "    ax5.grid(axis='y', alpha=0.3)\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax5.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{height:.1f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# 6. Energy vs Danceability by cluster\n",
        "ax6 = plt.subplot(2, 3, 6)\n",
        "if 'energy' in df_clean.columns and 'danceability' in df_clean.columns:\n",
        "    for cluster_id in range(optimal_k):\n",
        "        mask = cluster_labels == cluster_id\n",
        "        ax6.scatter(df_clean.loc[mask, 'danceability'], \n",
        "                   df_clean.loc[mask, 'energy'],\n",
        "                   label=f'Cluster {cluster_id}', alpha=0.5, s=20, edgecolors='black', linewidth=0.2)\n",
        "    ax6.set_xlabel('Danceability', fontsize=11, fontweight='bold')\n",
        "    ax6.set_ylabel('Energy', fontsize=11, fontweight='bold')\n",
        "    ax6.set_title('Energy vs Danceability by Cluster', fontsize=12, fontweight='bold')\n",
        "    ax6.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
        "    ax6.grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('Comprehensive Cluster Analysis Dashboard', fontsize=16, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nAdditional visualizations completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# STEP 12: FINAL SUMMARY\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"CLUSTERING ANALYSIS - FINAL SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nDataset Information:\")\n",
        "print(f\"  Total samples: {len(df_clean):,}\")\n",
        "print(f\"  Features used: {X_encoded.shape[1]}\")\n",
        "print(f\"    - Numerical features: {len(available_numerical)}\")\n",
        "print(f\"    - Encoded categorical features: {X_encoded.shape[1] - len(available_numerical)}\")\n",
        "\n",
        "print(f\"\\nClustering Results:\")\n",
        "print(f\"  Optimal number of clusters (k): {optimal_k}\")\n",
        "print(f\"  Final Inertia: {final_inertia:.2f}\")\n",
        "print(f\"  Final Silhouette Score: {final_silhouette:.3f}\")\n",
        "print(f\"  PCA Variance Explained: {total_variance*100:.2f}%\")\n",
        "\n",
        "print(f\"\\nCluster Distribution:\")\n",
        "cluster_counts = pd.Series(cluster_labels).value_counts().sort_index()\n",
        "for cluster_id, count in cluster_counts.items():\n",
        "    percentage = (count / len(cluster_labels)) * 100\n",
        "    print(f\"  Cluster {cluster_id}: {count:,} songs ({percentage:.2f}%)\")\n",
        "\n",
        "print(f\"\\nKey Insights:\")\n",
        "print(f\"  1. The elbow method and silhouette score were used to determine optimal k={optimal_k}\")\n",
        "print(f\"  2. Clusters are visualized in 2D using PCA dimensionality reduction\")\n",
        "print(f\"  3. Each cluster represents songs with similar audio characteristics\")\n",
        "print(f\"  4. The clustering can be used for music recommendation and genre analysis\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CLUSTERING ANALYSIS COMPLETE\")\n",
        "print(\"=\"*80)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mlcourse",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
